% The document class supplies options to control rendering of some standard
% features in the result.  The goal is for uniform style, so some attention 
% to detail is *vital* with all fields.  Each field (i.e., text inside the
% curly braces below, so the MEng text inside {MEng} for instance) should 
% take into account the following:
%
% - author name       should be formatted as "FirstName LastName"
%   (not "Initial LastName" for example),
% - supervisor name   should be formatted as "Title FirstName LastName"
%   (where Title is "Dr." or "Prof." for example),
% - degree programme  should be "BSc", "MEng", "MSci", "MSc" or "PhD",
% - dissertation title should be correctly capitalised (plus you can have
%   an optional sub-title if appropriate, or leave this field blank),
% - dissertation type should be formatted as one of the following:
%   * for the MEng degree programme either "enterprise" or "research" to
%     reflect the stream,
%   * for the MSc  degree programme "$X/Y/Z$" for a project deemed to be
%     X%, Y% and Z% of type I, II and III.
% - year              should be formatted as a 4-digit year of submission
%   (so 2014 rather than the accademic year, say 2013/14 say).

\documentclass[ % the name of the author
                    author={Gavin Parker},
                % the name of the supervisor
                supervisor={Dr. Neill Campbell},
                % the degree programme
                    degree={MEng},
                % the dissertation    title (which cannot be blank)
                     title={Deep Siamese Networks for Illumination Estimation from Stereo Images},
                % the dissertation subtitle (which can    be blank)
                  subtitle={},
                % the dissertation     type
                      type={research},
                % the year of submission
                      year={2018} ]{dissertation}

\begin{document}

% =============================================================================

% This section simply introduces the structural guidelines.  It can clearly
% be deleted (or commented out) if you use the file as a template for your
% own dissertation: everything following it is in the correct order to use 
% as is.

\section*{Prelude}
\thispagestyle{empty}

A typical dissertation will be structured according to (somewhat) standard 
sections, described in what follows.  However, it is hard and perhaps even 
counter-productive to generalise: the goal is {\em not} to be prescriptive, 
but simply to act as a guideline.  In particular, each page count given is
important but {\em not} absolute: their aim is simply to highlight that a 
clear, concise description is better than a rambling alternative that makes
it hard to separate important content and facts from trivia.

You can use this document as a \LaTeX-based~\cite{latexbook1,latexbook2} 
template for your own dissertation by simply deleting extraneous sections
and content; keep in mind that the associated {\tt Makefile} could be of
use, in particular because it automatically executes %\mbox{\BibTeX} to 
deal with the associated bibliography.  

You can, on the other hand, opt {\em not} to use this template; this is a 
perfectly acceptable approach.  Note that a standard cover and declaration 
of authorship may still be produced online via
\[
\mbox{\url{http://www.cs.bris.ac.uk/Teaching/Resources/cover.html}}
\]

% =============================================================================

% This macro creates the standard UoB title page by using information drawn
% from the document class (meaning it is vital you select the correct degree 
% title and so on).

\maketitle

% After the title page (which is a special case in that it is not numbered)
% comes the front matter or preliminaries; this macro signals the start of
% such content, meaning the pages are numbered with Roman numerals.

\frontmatter

% This macro creates the standard UoB declaration; on the printed hard-copy,
% this must be physically signed by the author in the space indicated.

\makedecl

% LaTeX automatically generates a table of contents, plus associated lists 
% of figures, tables and algorithms.  The former is a compulsory part of the
% dissertation, but if you do not require the latter they can be suppressed
% by simply commenting out the associated macro.

\tableofcontents
\listoffigures
\listoftables
\listofalgorithms
\lstlistoflistings

% The following sections are part of the front matter, but are not generated
% automatically by LaTeX; the use of \chapter* means they are not numbered.

% -----------------------------------------------------------------------------

\chapter*{Executive Summary}

\noindent
In this paper we present a system for estimating the lighting in photographs and video streams of real scenes,  to be used for the realistic rendering of virtual objects and characters. Augmented Reality is a growing topic in computer graphics and vision, that involves superimposing synthetic information and images on real scenes, often with the aim of giving the illusion that the object is part of the real world. Current AR solutions use geometry estimation to correctly place and scale the virtual objects to fit into the real scene, but do very little to render those objects with other scene intrinsics. Our system uses a Siamese Convolutional Neural Network to extract the lighting intensities and directions from objects within a scene, such that virtual objects can be re-rendered with realistic parameters on-the-fly.

The appearance of an object is made up of its material properties, its geometry and the lighting conditions, making estimation of any one of these factors from a single image a difficult task. Previous work has tackled lighting estimation by treating geometry estimation as a separate task, making use of depth cameras or known geometry. I have produced a CNN that relies on the relative invariance of these 3 unknowns with respect to time, by attempting to predict illumination from a series of views of an object.

\begin{quote}
My research hypothesis is that a siamese CNN provided with RGB stereo images, or a recurrent network provided with a video stream, will achieve similar results to Stamatios Georgoulis et al. while eliminating the need for explicit geometry estimation or the use of a depth camera.
\end{quote}

To test my hypothesis I performed research and implementation as follows:

\noindent
\begin{itemize}
\item Researched existing algorithms and neural network architectures for lighting and geometry estimation.
\item Replicated the work of Stamatios Georgoulis et al. by building CNNs that can interpolate sparse reflectance maps and predict environment map lighting from single objects with provided geometry.
\item Implemented a dataset generator that could produce realistic lighting parameters and images for training networks.
\item Created a new siamese architecture to encode geometry from stereo images and estimate lighting conditions
\end{itemize}

% -----------------------------------------------------------------------------

\chapter*{Supporting Technologies}

{\bf A compulsory section, of at most $1$ page}
\vspace{1cm} 

\noindent
This section should present a detailed summary, in bullet point form, 
of any third-party resources (e.g., hardware and software components) 
used during the project.  Use of such resources is always perfectly 
acceptable: the goal of this section is simply to be clear about how
and where they are used, so that a clear assessment of your work can
result.  The content can focus on the project topic itself (rather,
for example, than including ``I used \mbox{\LaTeX} to prepare my 
dissertation''); an example is as follows:

\begin{quote}
\noindent
\begin{itemize}
\item Tensorflow
\item OpenCV libraries
\item Blender
\item HDRIHaven
\item Shapenet
\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Notation and Acronyms}

{\bf An optional section, of roughly $1$ or $2$ pages}
\vspace{1cm} 


\begin{quote}
\noindent
\begin{tabular}{lcl}
AR                 &:     & Augmented Reality                                         \\
CNN                 &:     & Convolutional Neural Network                             \\
HDR					&:		& High Dynamic Range
\end{tabular}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Acknowledgements}

{\bf An optional section, of at most $1$ page}
\vspace{1cm} 

\noindent
It is common practice (although totally optional) to acknowledge any
third-party advice, contribution or influence you have found useful
during your work.  Examples include support from friends or family, 
the input of your Supervisor and/or Advisor, external organisations 
or persons who  have supplied resources of some kind (e.g., funding, 
advice or time), and so on.

% =============================================================================

% After the front matter comes a number of chapters; under each chapter,
% sections, subsections and even subsubsections are permissible.  The
% pages in this part are numbered with Arabic numerals.  Note that:
%
% - A reference point can be marked using \label{XXX}, and then later
%   referred to via \ref{XXX}; for example Chapter\ref{chap:context}.
% - The chapters are presented here in one file; this can become hard
%   to manage.  An alternative is to save the content in seprate files
%   the use \input{XXX} to import it, which acts like the #include
%   directive in C.

\mainmatter

% -----------------------------------------------------------------------------

\chapter{Contextual Background}
\label{chap:context}

{\bf A compulsory chapter,     of roughly $5$ pages}
\vspace{1cm} 
\section{Computer Graphics and Lighting}

\noindent
Traditional computer graphics involves creating virtual scenes, with geometry represented as a series of surfaces in 3D space. Each surface is given properties such as texture and material, and virtual lights are placed in the scene with given intensities and colors. Given the position and size of a virtual camera images can be produced by calculating the resultant color at every point on a the screen, produced by a combination of the lighting and surface properties. This can be achieved through either Raytracing which involves simulating the paths of all light rays that meet the camera within the scene. While physically accurate this is a slow process that is impractical for real-time rendering. In real-time rendering tasks a process called Rasterisation is used, where the geometry is culled so that only surfaces in view of the camera need to be considered. This, in combination with lighting and material approximations, makes it possible for convincing characters and objects to be rendered at 60 frames per second.
\newline
The material of a surface defines how it reflects or absorbs incoming light, and as a result characterizes its colour under different lighting conditions. In the physical world, no surface is perfectly even on a microscopic scale. When an incoming photon reaches a surface, its angle of reflection is actually dictated by whatever atoms it collides with, making per-photon reflections very difficult to predict even for very smooth surfaces. Furthermore many photons are actually absorbed by the material depending on it's colour properties. However, visible light is made up of a vast supply of photons, which can even be considered as a wave with properties like wavelength or amplitude. If we consider light on a macroscopic scale it is far easier to predict and represent. We often use the concept of 'colour' to refer to the wavelengths of light which an object reflects, where lighter objects reflect more wavelengths and darker objects reflect fewer and absorb more photons. It is essential to note that the colour something takes on is actually representative of the wavelengths of light that it reflects towards the light sensing device, be it an eye or a camera. Very smooth objects such as mirrors reflect most wavelengths, at very predictable angles, and so take on different colours depending on the viewpoint. These behaviours are captured in the 'Rendering Equation', 
\[L_0(x, w_0, lambda) = L_e(x, w_0, lambda) + \int_{ohm}{f_r(x, w_i, w_o,lambda)L_i(x, w_i, lambda)dw_i}\],
 \todo{Simplify this}
which Computer Graphics attempts to approximate.
\newline
In Computer Graphics, materials are often referred to as having 'Diffuse' and 'Specular' properties, which capture the aforementioned behaviour. This is in fact an approximation that allows for the efficient rendering of surfaces, though does not capture the light behaviour perfectly. 'Diffuse' refers to the wavelength, or colour, of light that is reflected off a surface equally in every direction. This can be thought of as the 'base colour', that is somewhat invariant to the viewpoint of the observer. 'Specularity' refers to how reflective an object is, and is often used to approximate the light being reflected directly from a light source towards the observer. The 'Phong Shading' model combines these two elements to roughly represent a variety of materials, under the assumption that reflective materials have a dominant component that can be modelled as a mirror. For example an unlaquered wooden panel will be very unreflective and may contain no 'specular' component at all as its colour remains fairly consistent when observed from every direction. On the other hand a plastic ball will have a dominant colour but also reflect a lot of the incoming light, which is modelled as a combination of the incoming light direction and the surface angle. Modern Raytracing makes use of a far more accurate model called a Bidirectional Reflectance Distribution Function, which defines a probability distribution function. This function gives the probablity of an incoming photon with angle of incidence i, reflecting with an angle of incidence j. Over many photons this is able to accurately capture the reflective behaviour of a surface, and is often used in combination with properties like 'Albedo' and 'Subsurface Scattering' to result in an almost photorealistic surface.
\newline
A product of the reflective nature of surfaces results in 'indirect lighting', an effect that is extremely difficult to capture in real-time rendering. Often when rendering is expensive it is appropriate to approximate the lighting in the scene to contain only 'direct' and 'environment' lighting. In this approximation, all objects are lit by the environment light, which represents an approximate light in all directions at all points in the scene. This is very cheap to render, as provided there is no direct lighting, all points on a surface recieve a constant amount of light. Direct lighting refers to lighting objects based on their proximimity to a chosen light source, ignoring shadows that would require calculating and intersections between the light and the surface. Unfortunately this approximation results in very unconvincing images as in reality the light at a point is made up of all the photons being reflected from that point. The Phong shading model ignores the effect of photons being reflected many times within a scene, and so misses much of the subtle detail. A common way of calculating indirect lighting is 'Global Illumination', which involves tracing the paths of many light rays from light sources as they are reflected within the scene. To save computation is is often performed in reverse, where rays are drawn from the camera and reflected a constant number of times. The illumination of the final reflection is then computed from the direct lighting, which is then passed between the reflected surfaces to achieve the approximate lighting at the original scene point. This is a very expensive procedure and innapropriate for real-time rendering, where other approximations can be used. One such approximation is an 'Environment Map' or 'Environment Probe', which is the precomputed representation of all the incident light at a single point. This can be thought of as a panorama image at a single point, capturing both the direct and indirect light. While this cannot be used for every point in the scene, it is fair to assume that nearby objects have similar incident lighting. A common application is to use a large environment map to represent the lighting in outdoor scenes, where the main light source is the sky. This results in a vast improvement over a single Environment/Direct light combination, while still being inexpensive to compute. Furthermore, multiple environment maps can be blended together to roughly represent the lighting across larger scenes, often in combination with other approximations such as baked lighting.
\newline
A benefit on 'Environment Maps' is that rather than being computed, they can be captured from real-world data. This is a technique often used in films to apply the lighting from a set to virtual additions. This is achieved by taking photographs of a reflective sphere under different exposures to capture a range of lighting intensities. These images can then be composited to capture the range of light in a single format. By using a sphere, which has known geometry, it is easy to invert the image into a High Dynamic Range panorama. It is essential to use an HDR format, as RGB captures only a very narrow range of light.
\section{Augmented Reality}
Augmented Reality is an extension of computer graphics, where virtual objects are rendered on a real video stream to appear as if they are part of the original scene. This creates additional challenges as many of the parameters that are used in computer graphics are not available. While the material and geometry of the added objects is known, the geometry and lighting of the real scene must either be estimated, or manually recorded beforehand. The latter option tends to be used in films, where the layout of the scene and camera movements are known beforehand. In this case the geometry of the scene can be measured and recreated in 3D modelling or CAD software for a virtual approximation. Similarly, the lighting at points in the scene can be captured using a light probe, often a reflective sphere, and taking composite photographs at different exposures. This process results in an 'Environment Map' which captures the color and intensity of all the incoming light at a single point. With this data, additional characters can be rendered entirely in the virtual space, interacting with the approximated geometry and being lit by blended combinations of light probes across the scene. Provided that the virtual camera moves exactly as the real camera, the added object can be masked and overlaid onto the original video of the scene.
\newline
In live AR, as is becomming common in mobile applications, the scene data must be approximated entirely from data gathered from the devices sensors. Geometry estimation is performed using a process known as Salient Localization and Matching, which can be used to track the movement of a camera through a 3d environment. SLAM relies on finding similar points from multiple camera views, with the assumption that these points share the same position in 3D space. Given known camera parameters such as focal length, and the relative position of the cameras between the views, it is then possible to triangulate these salient points to find their location. SLAM is used extensively in the field of robotics, often using stereo cameras such that scene points can be calculated at every frame of an incoming video stream. By tracking the movement of 3D points in a video, it is not only possible to track the cameras movements, but to also build a 3D mapping of the scene geometry. In the context of AR it is essential to track the cameras position as it corresponds to the users position within the Mixed Reality environment. Furthermore by building a 3d map of the environment it is possible to find appropriate surafces on which to place virtual objects, or even apply shadows.
\newline
Another scene parameter that must be estimated is the lighting. Unlike geometry estimation which has significant uses in the field of robotics, solutions for lighting estimation tend to be more rudimentary. For real-time estimation, especially on mobile devices, it is common to simply pick the fastest approximation to maximise performance. In fact, mobile graphics hardware is far less powerful than what is used in the film industry, meaning that even perfect lighting representations may result in an unrealistic image. Nonetheless, mobile graphics hardware continues to improve and the quality bottleneck for AR will eventually shift to lighting estimation. In the case of shadows and reflective objects, good lighting estimation is a necessity. One fast approach is to simply take an intensity average across each incoming video frame, and use this to determine the overall brightness of the image. This is very approximate, and ignores light direction and colour but means that superimposed objects do not appear overly bright in dark areas and visa versa. There are also some techniques that make use of machine learning to estimate incoming light direction given some basic constraints light a single light source (usually outside), which restricts the possible incoming light or strictly portrait images, which reduce the possible scene geometry and materials.
\newline
The difficulty of these tasks arises from the fact that lighting, material and geometry are linked unknown parameters, and so an understanding of one is required to estimate the other. For example a red sphere may appear so because it is painted red, or because it is reflective and the lighting of the scene is red in tone. In fact this problem is the basis for many optical illusions, as it is sometimes difficult for humans to determine what exactly they are seeing under certain conditions. Humans share most of the properties of a moving mobile AR device, yet are able to infer properties of the environment under most circumstances. We achieve this by making assumptions and approximations while also utilising prior knowledge and a semantic understanding of the world around us. It is these features that we must formalise and exploit if we are to build convincing AR tools.
\newline
One feature that humans and computer vision sytems can exploit is shadows, where the light from a light source is blocked from reaching it's destination. In the case that we have an object of consistent shape and material such as a flat table, it is sensible to treat significantly darker patches on the surface as shadows. If we can find the object that is casting the shadow, it can be trivial to triangulate the light source. However, this technique is only accurate if there are no invariances in lighting across the scene that could result in changes in brighness across surfaces without explicit shadows. This model is still useful however, especially in outdoor scenes where we can treat the sun as a single light source, approximately equidistant from every point in the scene. In this case, any darkening on objects of the same material must be caused by shadowing. In indoor scenes we cannot make this assumption; often there are multiple light sources, which are close enough to the objects in the scene to cause differences in direct lighting. However in indoor scenes we have other consistencies to benefit from such as more uniform geometry. When we consider indoor scenarios, especially those likely to be applicable to AR and film we find that there are regular flat surfaces such as floors and furniture that are reasonably consistent in geometry. While the assumption that the light source is infinitely far away doesn't apply here, we are able to restrict light sources to likely directions, such as artificial light from bulbs above or natural light from windows. While these assumptions can aid in lighting prediction, it is still extremely difficult to build a robust lighting estimation system for indoor scenes, resulting in the simplified techniques used in todays AR platforms.
\newline
Often the reason humans find it easy to understand the light in a scene is because of a semantic understanding. While we do not know the geometry of every room we enter, we are able to recognise consistencies that we have encountered before. While a single image patch could technically be any combination of material, light and geometry we can restrict the likely combinations by recollecting previous times we have seen something similar. We understand, for example that surfaces with dramatic variances in color and light intensity are more likely to be a reflective metal than to have those diffuse colour properties. Similarly if we see an orange tinted outdoor scene we know to assume that it is probably due to the color of the sky, because we recognise that known surfaces like grass or concrete are not naturally that colour.
\section{Machine Learning and Artificial Intelligence}
Because the complicated nature of semantics it is very difficult to create a system that explicitly relies on finding known patterns between scenes to cover all cases. For example it would be viable to create a system tht extracts IKEA furniture from scenes, and uses the known geometry and material to treat the furniture as sparse light probes. This would not be able to account for scenes without known furniture, outdoor scenes or even scenes where the furniture is partially obscured. Instead it is a sensible approach to consider machine learning. Machine Learning is a field of Computer Science that involves training the parameters of a system on data to make predictions. By taking known input and output pairs it is possible to adjust the parameters of the system such that it can generalise to inputs with unknown outputs. In our case it would be possible to train a model to take input images, either raw or preprocessed, and output some representation of the lighting. Traditional ML models such as the Support Vector Machine are able to learn a given number of parameters from some input features. These features could be extracted from the image, such as the average intensity or the dominant frequency in the foutier domain. These models are easy to train and can produce very predictable results, but require the designer to explicitly provide a set of features that they believe can be used to seprerate or define the different outputs. For example if our task was to predict whether a given image was 'indoors' or 'outdoors' we have restricted our problem to binary classification - our outputs are known. It would then be feasible to provide some features, such as average colour or number of colour clusters above a threshold to then train an SVM.
\newline
If we wish to predict a more complicated lighting model we must be able to take into account many more features than what we can feasibly define by hand. In this case we can look to the field of AI, and explicitly Convolutional Neural Networks. In the field of AI it is possible to train a model without explicit features, by using an approximation of neural dynamics in real brains. In broad terms, we programmatically create a series of connected neurons in a layered structure, with each neuron having a number of inputs and some number of outputs. The neurons have a very simple behaviour - they multiply each input by some 'weight' and provide the accumulated result as the output.
\[y_k = \sum_{j=0}^{m}{w_{kj}x_j}\]
 By training on known results we can adjust these weights so that the final neuron layer (which matches our desired output data shape) contains the desired output.
\newline
Convolutional Neural Networks are a recent development in AI that allows for reasonably fast AI models that take images as input. Rather than having a neuron for each colour of each pixel, we can instead learn the weightings for a 'convolution'. A convolution, in this context, is a function which can be passed across an image to extract features,
\[\sum_{n_1=-s}^{s}{\sum_{n_2=-s}^{s}{f[n_1, n_2]\cdot g[x-n_1,y-n_2]}} \]
and often resembles a SxS matrix. Convolutions are used frequently in computer vision, as they are an efficient way of computing many popular image features such as Harris Corners or Sobel edges. In a CNN we intend to learn the weights of many convolutional matrices to quickly extract featured from the input image. Furthermore we are able to pass further convolutions across the results from previous convolutional layers to learn more complex features. For example the first layer could extract edges while a second layer extracts the presence of 'X' shapes. CNNs make it possible to train deep and complex models on image data, resulting in robust models that often vastly outperform their traditional ML counterparts. The prime application of CNNs has been in image classification, where the model must distinguish between a set number of classes. However it is possible to use CNNs for regression tasks that output other images using the concept of 'deconvolutional' layers. These learn an interpolation matrix that increases the dimensionality of the input. Popular image regression architectures often involve a series of convolutional layers followed by a series of deconvolutional layers. The aim of the convolutional layers is to learn a deep representation of the content of the image, often removing the original spatial dimension. Rather than 'shallow' features like edges or colours, these features ideally represent som 'understanding' of the scene such as presence of some object or shape of the environment. The deconvolutional layers can then interpolate this data into a new image that will often share many common features with the original. For example in style transfer, the convolutional layers attempt to extract the content of the image while the deconvolutional layers interpolate the image features with weights representing some artistic style.
\newline
The difficulty in the application of CNNs often lies in the 'training' stage; while a deep enough network should be able to learn a mapping for any function, the difficulty in providing data to capture every case increases with the problem complexity. If a network is trained to recognize images of dogs, it will only be able to learn features from dogs that it has seen as input. As a result, if a new dog is provided that lacks some of the features of the training data, it may be missclassified. Regression tasks in particular attempt to learn a complex funtion and so need a vast amount of training data to be able to generalise. This becomes problematic where the training data is hard to collect and classify. In the case of image classification, many photographs can be taken and marked with their intended class. With image regression this can become an extremely laborious task as the desired output needs to be extracted by hand. Often there are large datasets already available that can help speed up the process, especially if the datasets are labelled, or if features can be extracted by hand. However it is simply necessary to manually create a dataset to train a network to face a new task. Fortunately there are still some steps that can be taken to accelerate this process, such as data augmentation. This usually involves manipulating some training inputs in such a way as to introduce variances while maintaining the classification, and can greatly increase the amount of data available. Another approach is to create synthetic virtual data, by rendering new images. It is important to note however that the synthetic data must be accurate enough such that a model trained on it can generalise to real world data. It is often sensible to mix real world and synthetic data when training a model for the best results.


% -----------------------------------------------------------------------------

\chapter{Technical Background}
\label{chap:technical}
\section{Commercial Lighting Solutions}
Lighting solutions in the film\&TV industry tend to involve precalculating or precapturing the lighting in a scene before adding virtual characters. In the film 'I,Robot', Digital Domain created a semi-automated system for capturing HDR environment maps using a rotating fisheye-lens camera. Once placed, the camera was able to take shots at multiple angles and at a range of exposures, before compositing the shots together to produce a usable HDR lighting map. Often these HDR maps are not used in isolation, but along with footage of a proxy for the virtual addition, so the filmmakers can qualitatively measure how the materials react with the set lighting. These techniques allow filmmakers to capture an approximation of lighting with limited camera equipment and makes it easier to create complex panning and dolly shots of scenes with real and virtual elements, which would otherwise require a green-screen. Over time, camera equipment has become more advanced and it is faster to take high quality shots at multiple exposures. As a result, later films such as 'The Curious Case of Benjamin Button' are able to utilise multiple lighting probes to illuminate characters. By taking lighting samples across the scene, virtual characters can then be lit partly according to their position in the world by blending environment maps together. This creates a more convincing final render as the approximate indirect lighting from nearby objects becomes more apparent. The downside of these hollywood techniques is that the lighting data must be captured beforehand for every environment used. This is a long, laborious and expensive process as the cameras required to take accurate shots at different exposures need to be extremely accurate to produce correct composites. It is also important to note that compositing images is difficult and also requires human input to match black levels, tones and camera properties that are hard to automate.
\newline
Recent commercial AR platforms do contain some level of lighting estimation from input video data. Apple's ARKit is able to estimate the colour and intensity of the ambient lighting in some scenes \footcite{https://developer.apple.com/documentation/arkit/arlightestimate}, by analyzing the pixels of the incoming video frame. This technique adds some realism to AR applications, as objects can be dimmed or brightened with the scene, so as not to stand out. However this is a very rough estimate of lighting, and contains no directional information or approximation of indirect illumination  Google's ARCore has similar functionality, taking an average of the luminance values across the video to estimate the intensity \footcite{https://developers.google.com/ar/reference/unity/prefab/Environmental_Light}. It is possible to estimate the light direction from a face tracking scene however, by using the tracked face as a light probe \footcite{https://developer.apple.com/documentation/arkit/ardirectionallightestimate}. By using a face, the application is able to restrict the possible lighting scenearios to a range of likely geometries and materials that make up human faces. The platform is able to produce not only the primary light direction and intensity, but also an approximation of the environment lighting as spherical harmonics. While incredibly useful for some application such as the retouching of portrail photographs, the technique is not robust to different scenes as it relies on the approximation of some known geometry.
\newline
Upcoming AR applications are experimenting with new methods of integrating the scene lighting with virtual augmentations. Magic Leap is making use of tailored hardware called a 'Light Field' camera, which involves the use of an array of microlenses to capture additional information from the scene. Each microlens is able to capture information from multiple camera points, so that extra depth and lighting information can be extracted from the frame. This is a significant improvement on IR depth cameras such as the Microsoft Kinect, which can't be used outside. By extracting depth, and therefore surface geometry information, it is far easier to reason about the reflectance of objects within the scene and estimate the lighting. However, light-field cameras are extremely expensive and inpractical for mobile AR. Most mobile devices contain normal high quality cameras as well as accurate intertia sensors, which can be used in combination with stereo matching for a similar depth estimation model.
\section{Lighting Estimation Research}
Early techniques for lighting estimation were able to find the direction of a single primary light source, under significant constraints. In \footcite{http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=990650}, Peter Nillius and Jan-Olof Eklundh were able to estimate a single light direction, provided the input image contained an appropriate lambertian surface. By making use of an 'occluding contour' it is possible to infer some detail of the geometry, as the surface of the visible edge of a contour that occludes itself will always be perpendicular to the view direction of the observer. Using the lambertian lighting model, along with the edge intensity it is possible to make a prediction using a bayesian network for the incoming light direction. This is a simple method that exploits inherent properties of real geometry, but has too many prerequisites to be practical. While it captures direction, it also ignores colour, intensity and any indirect lighting. 
\todo{More basic stuff here}
In \cite{Lalonde-2009-10350}, Lalonde et al. are able to estimate a more complete set of parameters for environmental lighting from single outdoor photographs. By segmenting the image into sky, ground and vertical surfaces, and extracting features like shadows, the authors are able to reliable predict the sun's position. While very robust for outdoor applications, this approach does not consider light intensity or colour. Furthermore, the reliance on outdoor scenes restricts its usage in AR.
Scott Wehrwein et al. demonstrate a technique for sun direction and shadow detection from multiple input images \cite{http://www.cs.cornell.edu/projects/shadows/files/wehrwein_3dv15_shadows.pdf}. They use illunination rations to extract shadows, after constructing a sparse 3D representation of the scene from stereo matching of multiple images. By exploiting shadow boundaries on the derived surface normals, it is then possible to determine the light direction. This work shows how multiple views can be used to infer knowledge of the geometry of the scene, to make the problem of lighting estimation more constrained. However, like previous work it assumes a perfectly lambertian surface and clear outside conditions, restricting the possible lighting conditions significantly.

\footcite{http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=790313}
\todo{Talk about CNNs for estimating geometry/material}
\section{Machine Learning for Illumination Estimation}
Most of the previous work has had to rely on significant assumptions for the scene to constrain the lighting to fit known phenomena, that can be exploited to estimate the illumination. It is clear that due to the unconstrained nature of the problem, that some assumptions about geometry, material and lighting must be made. Rather than making the required assumptions to fit a model, another approach is building a model to find and exploit assumptions from real data using machine learning. Given enough scene features and data it is possible to learn approximations and assumptions to produce a more robust model. In \footcite{https://arxiv.org/pdf/1611.06403.pdf}, Yannick Hold-Geoffroy et al. use a CNN-based technique to estimate sky parameters from outdoor photographs to a high enough degree of accuracy that virtual objects can be re-rendered. In this work, the output illumination model is constrained to the Hosek-Wilkie model with parameters representing the sun position, ground albedo and light wavelength. A dataset is then constructed by selecting small LDR sections of real HDR scene panoramas, resulting in a large number of sample photographs, and HDR lighting ground-truths. This work demonstrates extraordinary performance, but is limited in output format by the lighting model which is constrained to outdoor images with clear skies. This ignores the indirect lighting from scene objects and indoor scenes, but demonstrates how CNNs with the right data can be used to exploit more subtle image features to predict lighting.
\todo{talk about this awesome microsoft thingy for recurrency etc. http://www.cs.wm.edu/~ppeers/showPublication.php?id=Xia:2016:RSS}
\begin{enumerate}
\item Commercial lighting modelling
\item Previous stochastic lighting estimation.
\item Previous ML lighting estimation
\item Previous CNN lighting estimation
\item Prior art
\end{enumerate}

% -----------------------------------------------------------------------------

\chapter{Project Execution}
\label{chap:execution}

{\bf A topic-specific chapter, of roughly $15$ pages} 
\vspace{1cm} 

\noindent
This chapter is intended to describe what you did: the goal is to explain
the main activity or activities, of any type, which constituted your work 
during the project.  The content is highly topic-specific, but for many 
projects it will make sense to split the chapter into two sections: one 
will discuss the design of something (e.g., some hardware or software, or 
an algorithm, or experiment), including any rationale or decisions made, 
and the other will discuss how this design was realised via some form of 
implementation.  

This is, of course, far from ideal for {\em many} project topics.  Some
situations which clearly require a different approach include:

\begin{itemize}
\item In a project where asymptotic analysis of some algorithm is the goal,
      there is no real ``design and implementation'' in a traditional sense
      even though the activity of analysis is clearly within the remit of
      this chapter.
\item In a project where analysis of some results is as major, or a more
      major goal than the implementation that produced them, it might be
      sensible to merge this chapter with the next one: the main activity 
      is such that discussion of the results cannot be viewed separately.
\end{itemize}

\noindent
Note that it is common to include evidence of ``best practice'' project 
management (e.g., use of version control, choice of programming language 
and so on).  Rather than simply a rote list, make sure any such content 
is useful and/or informative in some way: for example, if there was a 
decision to be made then explain the trade-offs and implications 
involved.

\section{Example Section}

\subsection{Example Sub-section}


\subsubsection{Example Sub-sub-section}

This is an example sub-sub-section;
the following content is auto-generated dummy text.
\lipsum

\paragraph{Example paragraph.}

This is an example paragraph; note the trailing full-stop in the title,
which is intended to ensure it does not run into the text.

% -----------------------------------------------------------------------------

\chapter{Critical Evaluation}
\label{chap:evaluation}

{\bf A topic-specific chapter, of roughly $15$ pages} 
\vspace{1cm} 

\noindent
This chapter is intended to evaluate what you did.  The content is highly 
topic-specific, but for many projects will have flavours of the following:

\begin{enumerate}
\item functional  testing, including analysis and explanation of failure 
      cases,
\item behavioural testing, often including analysis of any results that 
      draw some form of conclusion wrt. the aims and objectives,
      and
\item evaluation of options and decisions within the project, and/or a
      comparison with alternatives.
\end{enumerate}

\noindent
This chapter often acts to differentiate project quality: even if the work
completed is of a high technical quality, critical yet objective evaluation 
and comparison of the outcomes is crucial.  In essence, the reader wants to
learn something, so the worst examples amount to simple statements of fact 
(e.g., ``graph X shows the result is Y''); the best examples are analytical 
and exploratory (e.g., ``graph X shows the result is Y, which means Z; this 
contradicts [1], which may be because I use a different assumption'').  As 
such, both positive {\em and} negative outcomes are valid {\em if} presented 
in a suitable manner.

% -----------------------------------------------------------------------------

\chapter{Conclusion}
\label{chap:conclusion}

{\bf A compulsory chapter,     of roughly $5$ pages} 
\vspace{1cm} 

\noindent
The concluding chapter of a dissertation is often underutilised because it 
is too often left too close to the deadline: it is important to allocation
enough attention.  Ideally, the chapter will consist of three parts:

\begin{enumerate}
\item (Re)summarise the main contributions and achievements, in essence
      summing up the content.
\item Clearly state the current project status (e.g., ``X is working, Y 
      is not'') and evaluate what has been achieved with respect to the 
      initial aims and objectives (e.g., ``I completed aim X outlined 
      previously, the evidence for this is within Chapter Y'').  There 
      is no problem including aims which were not completed, but it is 
      important to evaluate and/or justify why this is the case.
\item Outline any open problems or future plans.  Rather than treat this
      only as an exercise in what you {\em could} have done given more 
      time, try to focus on any unexplored options or interesting outcomes
      (e.g., ``my experiment for X gave counter-intuitive results, this 
      could be because Y and would form an interesting area for further 
      study'' or ``users found feature Z of my software difficult to use,
      which is obvious in hindsight but not during at design stage; to 
      resolve this, I could clearly apply the technique of Smith [7]'').
\end{enumerate}

% =============================================================================

% Finally, after the main matter, the back matter is specified.  This is
% typically populated with just the bibliography.  LaTeX deals with these
% in one of two ways, namely
%
% - inline, which roughly means the author specifies entries using the 
%   \bibitem macro and typesets them manually, or
% - using BiBTeX, which means entries are contained in a separate file
%   (which is essentially a databased) then inported; this is the 
%   approach used below, with the databased being dissertation.bib.
%
% Either way, the each entry has a key (or identifier) which can be used
% in the main matter to cite it, e.g., \cite{X}, \cite[Chapter 2}{Y}.

\backmatter

% -----------------------------------------------------------------------------

% The dissertation concludes with a set of (optional) appendicies; these are 
% the same as chapters in a sense, but once signaled as being appendicies via
% the associated macro, LaTeX manages them appropriatly.

\appendix

\chapter{An Example Appendix}
\label{appx:example}

Content which is not central to, but may enhance the dissertation can be 
included in one or more appendices; examples include, but are not limited
to

\begin{itemize}
\item lengthy mathematical proofs, numerical or graphical results which 
      are summarised in the main body,
\item sample or example calculations, 
      and
\item results of user studies or questionnaires.
\end{itemize}

\noindent
Note that in line with most research conferences, the marking panel is not
obliged to read such appendices.

% =============================================================================

\end{document}
